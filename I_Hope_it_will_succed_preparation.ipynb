{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-19T12:06:07.259269Z",
     "start_time": "2024-10-19T12:06:04.539332Z"
    }
   },
   "source": [
    "import torch\n",
    "import json\n",
    "\n",
    "# Load the extracted features (list of tensors)\n",
    "all_features = torch.load('image_features.pt', weights_only=True)  # This is a list of tensors\n",
    "image_name_to_index = torch.load('image_name_to_index.pt', weights_only=True)\n",
    "\n",
    "# Load the captions dictionary\n",
    "with open('captions_dict.json', 'r') as f:\n",
    "    caption_sequences = json.load(f)\n",
    "\n",
    "# Load the vocabulary\n",
    "with open('vocabulary.json', 'r') as vocab_file:\n",
    "    vocabulary = json.load(vocab_file)\n",
    "\n",
    "# Debug prints to confirm loading\n",
    "print(f\"Loaded {len(all_features)} features.\")\n",
    "print(f\"Loaded {len(caption_sequences)} captions.\")\n",
    "print(f\"Loaded {len(vocabulary)} words in the vocabulary.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8091 features.\n",
      "Loaded 8091 captions.\n",
      "Loaded 8517 words in the vocabulary.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T12:06:07.330024Z",
     "start_time": "2024-10-19T12:06:07.322123Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pad_captions(captions, max_length=21):\n",
    "    # Convert list of caption sequences into a tensor\n",
    "    caption_tensors = [torch.tensor(caption) for caption in captions]\n",
    "\n",
    "    padded_captions = []\n",
    "    for tensor in caption_tensors:\n",
    "        # If the caption is shorter than max_length, pad with zeros\n",
    "        if len(tensor) < max_length:\n",
    "            padding = torch.zeros(max_length - len(tensor), dtype=torch.long)\n",
    "            padded_captions.append(torch.cat((tensor, padding)))\n",
    "        else:\n",
    "            # If longer than max_length, truncate it\n",
    "            padded_captions.append(tensor[:max_length])\n",
    "\n",
    "    return torch.stack(padded_captions)"
   ],
   "id": "261590a0cb3063d7",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T12:06:07.560285Z",
     "start_time": "2024-10-19T12:06:07.541796Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Separate features and captions\n",
    "feature_caption_pairs = []\n",
    "for image_id, captions in caption_sequences.items():\n",
    "    image_feature = all_features[image_name_to_index[image_id]]  # Get the feature for the image\n",
    "    for caption in captions:\n",
    "        feature_caption_pairs.append((image_feature, caption))  # Create pairs\n",
    "\n",
    "# Now feature_caption_pairs contains tuples of (feature, caption)\n"
   ],
   "id": "bbc1e0d096b34365",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T12:22:56.161072Z",
     "start_time": "2024-10-19T12:22:56.147078Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "def data_generator(dataset, batch_size, max_length=21):\n",
    "    while True:\n",
    "        # Create a shuffled index list\n",
    "        indices = list(range(len(dataset)))  # Indices of the dataset\n",
    "        random.shuffle(indices)  # Shuffle indices\n",
    "        \n",
    "        for start in range(0, len(indices), batch_size):\n",
    "            end = min(start + batch_size, len(indices))\n",
    "            batch_indices = indices[start:end]  # Get shuffled indices for the current batch\n",
    "\n",
    "            # Separate features and captions using the shuffled indices\n",
    "            batch_pairs = [dataset[i] for i in batch_indices]  # Get pairs from the dataset\n",
    "            \n",
    "            batch_features = torch.stack([pair[0] for pair in batch_pairs])\n",
    "            batch_captions = [pair[1] for pair in batch_pairs]\n",
    "\n",
    "            # Pad the captions\n",
    "            padded_batch_captions = pad_captions(batch_captions, max_length)\n",
    "\n",
    "            yield batch_features, padded_batch_captions"
   ],
   "id": "acd81c045e93f3a1",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T12:22:56.860625Z",
     "start_time": "2024-10-19T12:22:56.832983Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test the loading part\n",
    "print(f\"Loaded {len(all_features)} features.\")\n",
    "print(f\"Loaded {len(caption_sequences)} captions.\")\n",
    "print(f\"Toatal image caption pairs {len(feature_caption_pairs)}\")\n",
    "# Test the data generator\n",
    "batch_size = 32\n",
    "data_gen = data_generator(feature_caption_pairs, batch_size)\n",
    "\n",
    "# Retrieve a batch\n",
    "batch_features, padded_batch_captions = next(data_gen)\n",
    "print(f\"Batch features shape: {len(batch_features)}\")\n",
    "print(f\"Padded captions shape: {padded_batch_captions.shape}\")\n",
    "print(f\"Example caption (first caption in the batch): {padded_batch_captions[0]}\")\n"
   ],
   "id": "233eb1a51404f353",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8091 features.\n",
      "Loaded 8091 captions.\n",
      "Toatal image caption pairs 38008\n",
      "Batch features shape: 32\n",
      "Padded captions shape: torch.Size([32, 21])\n",
      "Example caption (first caption in the batch): tensor([   1,   74,  202,    1,    2,   37,   78, 1269,   77, 1264,    1,  923,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0])\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T12:22:57.601361Z",
     "start_time": "2024-10-19T12:22:57.593854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# Total number of feature-caption pairs\n",
    "total_pairs = len(feature_caption_pairs)\n",
    "\n",
    "# Define split ratio (80% train, 20% test)\n",
    "train_size = int(0.8 * total_pairs)\n",
    "test_size = total_pairs - train_size\n",
    "\n",
    "# Split the data\n",
    "train_data, test_data = random_split(feature_caption_pairs, [train_size, test_size])\n",
    "\n",
    "# Now train_data and test_data contain the splits\n",
    "print(f\"Training data size: {len(train_data)}\")\n",
    "print(f\"Testing data size: {len(test_data)}\")"
   ],
   "id": "33e7a210d34f4b1e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: 30406\n",
      "Testing data size: 7602\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T12:42:32.762911Z",
     "start_time": "2024-10-19T12:42:32.744904Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ShowAndTellModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, lstm_units, feature_size):\n",
    "        super(ShowAndTellModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim + feature_size, lstm_units, batch_first=True, dropout=0.3)\n",
    "        self.fc = nn.Linear(lstm_units, vocab_size)\n",
    "\n",
    "    def forward(self, image_features, captions):\n",
    "        # Embed the captions\n",
    "        caption_embeddings = self.embedding(captions)\n",
    "        \n",
    "        # Repeat the image features for each time step\n",
    "        image_features_repeated = image_features.unsqueeze(1).repeat(1, captions.size(1), 1)\n",
    "        \n",
    "        # Combine the image features with the caption embeddings\n",
    "        combined_input = torch.cat((image_features_repeated, caption_embeddings), dim=2)\n",
    "        \n",
    "        # Pass through LSTM\n",
    "        lstm_out, _ = self.lstm(combined_input)\n",
    "        \n",
    "        batch_size, seq_len, hidden_size = lstm_out.size()\n",
    "        \n",
    "        \n",
    "        # Get the output for the last time step\n",
    "        output = self.fc(lstm_out)\n",
    "        \n",
    "        return output\n"
   ],
   "id": "5d245046349bd41e",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T12:42:33.557451Z",
     "start_time": "2024-10-19T12:42:33.551783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vocab_size = len(vocabulary) + 1\n",
    "embedding_dim = 256\n",
    "lstm_units = 512\n",
    "feature_size = 1000"
   ],
   "id": "936fcdf620627e10",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T12:42:34.309827Z",
     "start_time": "2024-10-19T12:42:34.245662Z"
    }
   },
   "cell_type": "code",
   "source": "model = ShowAndTellModel(vocab_size, embedding_dim, lstm_units, feature_size)",
   "id": "86dc670ddc911b9e",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T12:42:34.764908Z",
     "start_time": "2024-10-19T12:42:34.679863Z"
    }
   },
   "cell_type": "code",
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ],
   "id": "36e750dc9d203933",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T13:01:30.901396Z",
     "start_time": "2024-10-19T12:42:35.417175Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "train_gen = data_generator(train_data, batch_size)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for step in range(len(train_data) // batch_size):  # Define steps for one epoch\n",
    "        batch_features, padded_batch_captions = next(train_gen)  # Get the next batch\n",
    "\n",
    "        optimizer.zero_grad()  # Clear previous gradients\n",
    "\n",
    "        # Forward pass: Predict the caption sequence\n",
    "        outputs = model(batch_features, padded_batch_captions)  # Pass both features and captions\n",
    "\n",
    "        # Reshape the outputs and target captions for loss calculation\n",
    "        outputs = outputs.view(-1, outputs.size(-1))  # Shape: [batch_size * seq_len, vocab_size]\n",
    "        target_captions = padded_batch_captions.view(-1)  # Shape: [batch_size * seq_len]\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, target_captions)  # Use the reshaped outputs and target captions\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update the weights\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print statistics for the epoch\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / (len(train_data) // batch_size):.4f}\")"
   ],
   "id": "4d0cd326994cc479",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.5854\n",
      "Epoch [2/10], Loss: 0.0651\n",
      "Epoch [3/10], Loss: 0.0126\n",
      "Epoch [4/10], Loss: 0.0005\n",
      "Epoch [5/10], Loss: 0.0002\n",
      "Epoch [6/10], Loss: 0.0001\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[48], line 24\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;66;03m# Compute the loss\u001B[39;00m\n\u001B[0;32m     23\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs, target_captions)  \u001B[38;5;66;03m# Use the reshaped outputs and target captions\u001B[39;00m\n\u001B[1;32m---> 24\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Backpropagation\u001B[39;00m\n\u001B[0;32m     25\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()  \u001B[38;5;66;03m# Update the weights\u001B[39;00m\n\u001B[0;32m     27\u001B[0m running_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[1;32mV:\\Codes\\Python\\ImageCaption\\.venv\\Lib\\site-packages\\torch\\_tensor.py:521\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    511\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    512\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    513\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    514\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    519\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    520\u001B[0m     )\n\u001B[1;32m--> 521\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    522\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    523\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mV:\\Codes\\Python\\ImageCaption\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:289\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    284\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    286\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    287\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    288\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 289\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    290\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    291\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    292\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    293\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    294\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    295\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    296\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    297\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mV:\\Codes\\Python\\ImageCaption\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:769\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[1;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[0;32m    767\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[0;32m    768\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 769\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    770\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[0;32m    771\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    772\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    773\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "87ccd43499efef15"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
