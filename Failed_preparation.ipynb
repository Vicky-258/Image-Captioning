{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-19T08:47:19.099131Z",
     "start_time": "2024-10-19T08:47:17.585342Z"
    }
   },
   "source": [
    "import torch\n",
    "import json\n",
    "\n",
    "# Load the extracted features (list of tensors)\n",
    "all_features = torch.load('image_features.pt', weights_only=True)  # This is a list of tensors\n",
    "image_name_to_index = torch.load('image_name_to_index.pt', weights_only=True)\n",
    "\n",
    "# Load the captions dictionary\n",
    "with open('captions_dict.json', 'r') as f:\n",
    "    caption_sequences = json.load(f)\n",
    "\n",
    "# Load the vocabulary\n",
    "with open('vocabulary.json', 'r') as vocab_file:\n",
    "    vocabulary = json.load(vocab_file)\n",
    "\n",
    "# Debug prints to confirm loading\n",
    "print(f\"Loaded {len(all_features)} features.\")\n",
    "print(f\"Loaded {len(caption_sequences)} captions.\")\n",
    "print(f\"Loaded {len(vocabulary)} words in the vocabulary.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8091 features.\n",
      "Loaded 8091 captions.\n",
      "Loaded 8517 words in the vocabulary.\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T08:35:14.751697Z",
     "start_time": "2024-10-19T08:35:13.227660Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 3,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lengths = [len(caption) for caption in caption_sequences]\n",
    "plt.hist(lengths, bins=30)\n",
    "plt.xlabel('Caption Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Caption Lengths')\n",
    "plt.show()\n"
   ],
   "id": "b7d0b644fe244ecc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T08:35:16.064355Z",
     "start_time": "2024-10-19T08:35:16.051409Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check the number of captions for each image\n",
    "caption_counts = [len(captions) for captions in caption_sequences.values()]\n",
    "\n",
    "# Print the total number of images and the count of captions for the first 10 images\n",
    "print(f\"Number of images: {len(caption_counts)}\")\n",
    "print(f\"Number of captions per image (first 10): {caption_counts[:10]}\")\n"
   ],
   "id": "c025a08f586355b2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 8091\n",
      "Number of captions per image (first 10): [5, 5, 5, 5, 5, 5, 5, 3, 5, 5]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T08:35:18.854119Z",
     "start_time": "2024-10-19T08:35:17.135098Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def pad_captions(caption_sequences, max_length=21):\n",
    "    # Determine the maximum number of captions for any image\n",
    "    max_num_captions = max(len(captions) for captions in caption_sequences.values())\n",
    "    \n",
    "    padded_captions = []\n",
    "    \n",
    "    for captions in caption_sequences.values():\n",
    "        padded_for_image = []\n",
    "        \n",
    "        for caption in captions:\n",
    "            # Convert caption to tensor\n",
    "            tensor_caption = torch.tensor(\n",
    "                [int(word) if isinstance(word, str) and word.isdigit() else word for word in caption],\n",
    "                dtype=torch.long\n",
    "            )\n",
    "            # Truncate or pad\n",
    "            if tensor_caption.size(0) < max_length:\n",
    "                tensor_caption = torch.cat((tensor_caption, torch.zeros(max_length - tensor_caption.size(0), dtype=torch.long)))\n",
    "            else:\n",
    "                tensor_caption = tensor_caption[:max_length]  # truncate if too long\n",
    "            \n",
    "            padded_for_image.append(tensor_caption)\n",
    "\n",
    "        # Pad the list of captions to the maximum number of captions for this image\n",
    "        padded_for_image = pad_sequence(padded_for_image, batch_first=True)\n",
    "        \n",
    "        # If there are fewer captions than the max, pad the whole tensor to the maximum number of captions\n",
    "        if padded_for_image.size(0) < max_num_captions:\n",
    "            # Pad with zeros for missing captions\n",
    "            padding_needed = max_num_captions - padded_for_image.size(0)\n",
    "            padded_for_image = torch.cat((padded_for_image, torch.zeros(padding_needed, max_length, dtype=torch.long)))\n",
    "\n",
    "        padded_captions.append(padded_for_image)\n",
    "\n",
    "    # Convert to a single tensor\n",
    "    return torch.stack(padded_captions)\n",
    "\n",
    "max_length = 21\n",
    "padded_captions = pad_captions(caption_sequences, max_length)\n",
    "print(f\"Padded captions shape: {padded_captions.shape}\")  # Should be (num_images, max_num_captions, max_length)\n"
   ],
   "id": "19e9469766aa6224",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded captions shape: torch.Size([8091, 5, 21])\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T08:35:19.794954Z",
     "start_time": "2024-10-19T08:35:19.776366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check the number of features\n",
    "num_features = len(all_features)\n",
    "print(f\"Number of features: {num_features}\")\n",
    "\n",
    "# Check the number of padded captions\n",
    "num_padded_captions = len(padded_captions)  # This should be a list now\n",
    "print(f\"Number of padded captions: {num_padded_captions}\")\n",
    "\n",
    "# Check the shape of the first padded caption (if they all have the same shape)\n",
    "if num_padded_captions > 0:\n",
    "    first_caption_shape = padded_captions[0][0].shape  # Get the shape of the first padded caption tensor\n",
    "    print(f\"Shape of the first padded caption: {first_caption_shape}\")\n",
    "\n",
    "# Ensure they match the number of images\n",
    "if num_features == len(caption_sequences):\n",
    "    print(\"The number of features matches the number of images.\")\n",
    "else:\n",
    "    print(\"Mismatch detected between features and images!\")\n"
   ],
   "id": "be4ae07154358635",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 8091\n",
      "Number of padded captions: 8091\n",
      "Shape of the first padded caption: torch.Size([21])\n",
      "The number of features matches the number of images.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T08:35:20.945688Z",
     "start_time": "2024-10-19T08:35:20.932236Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ImageCaptionDataset(Dataset):\n",
    "    def __init__(self, features, padded_captions, name_to_index, transform=None):\n",
    "        self.features = features  # List of feature tensors\n",
    "        self.padded_captions = padded_captions  # List of lists for padded captions\n",
    "        self.name_to_index = name_to_index  # Dictionary mapping image names to indices\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)  # Return the number of images\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_feature = self.features[idx]  # Access feature directly as it's a list\n",
    "        caption = self.padded_captions[idx]  # Index padded captions list using the same idx\n",
    "\n",
    "        if self.transform:\n",
    "            image_feature = self.transform(image_feature)\n",
    "\n",
    "        return image_feature, caption\n"
   ],
   "id": "5084872bcccba9b6",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T08:35:24.852994Z",
     "start_time": "2024-10-19T08:35:21.689099Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 8091\n",
      "Number of padded captions: 8091\n",
      "Training set size: 6472\n",
      "Testing set size: 1619\n"
     ]
    }
   ],
   "execution_count": 8,
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create a list of image names to ensure proper alignment\n",
    "image_names = list(caption_sequences.keys())  # Get all image names\n",
    "\n",
    "# Create corresponding features and captions lists\n",
    "features_list = [all_features[i] for i in range(len(image_names))]\n",
    "captions_list = [caption_sequences[name] for name in image_names]  # Ensure alignment with features\n",
    "\n",
    "# Now pad the captions and convert them to tensors\n",
    "max_length = 21  # Set this to your desired max length\n",
    "padded_captions = pad_captions(caption_sequences, max_length)\n",
    "\n",
    "# Check if we have the same number of features and images\n",
    "print(f\"Number of features: {len(features_list)}\")  # Should be 8091\n",
    "print(f\"Number of padded captions: {len(padded_captions)}\")  # Should also be 8091\n",
    "\n",
    "# Create the dataset\n",
    "dataset = ImageCaptionDataset(features_list, padded_captions, image_name_to_index)\n",
    "\n",
    "# Split the dataset (80% train, 20% test)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Print the sizes of the training and testing sets\n",
    "print(f\"Training set size: {len(train_dataset)}\")\n",
    "print(f\"Testing set size: {len(test_dataset)}\")\n",
    "\n"
   ],
   "id": "5597436670776dc3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T08:35:26.321523Z",
     "start_time": "2024-10-19T08:35:26.313900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def collate_fn(batch):\n",
    "    features, captions = zip(*batch)  # Unzip the batch into features and captions\n",
    "    features = torch.stack(features)  # Convert features to a tensor\n",
    "    padded_captions = torch.nn.utils.rnn.pad_sequence(captions, batch_first=True)  # Pad captions\n",
    "    return features, padded_captions\n"
   ],
   "id": "be2e044f59313f1e",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T08:35:27.275819Z",
     "start_time": "2024-10-19T08:35:27.255829Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create data loaders for training and testing sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n"
   ],
   "id": "fdc0d2f372a1d572",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T08:35:28.000529Z",
     "start_time": "2024-10-19T08:35:27.984422Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ShowTellModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout_prob):\n",
    "        super(ShowTellModel, self).__init__()\n",
    "\n",
    "        # Image feature embedding\n",
    "        self.img_fc = nn.Linear(1000, embedding_dim)  # Ensure feature_dim is 1000 or adjust accordingly\n",
    "\n",
    "        # Word embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # LSTM for caption generation\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, dropout=dropout_prob, batch_first=True)\n",
    "\n",
    "        # Fully connected layer to map LSTM output to the vocabulary size\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "        # Dropout layer (optional)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, image_features, captions):\n",
    "        # Process image features\n",
    "        img_embedding = self.img_fc(image_features).unsqueeze(1)  # [batch_size, 1, embedding_dim]\n",
    "        \n",
    "        # Process caption embeddings\n",
    "        cap_embedding = self.embedding(captions)  # [batch_size, num_captions, seq_length, embedding_dim]\n",
    "        \n",
    "        # Print shapes for debugging\n",
    "        print(f\"Image embedding shape: {img_embedding.shape}\")\n",
    "        print(f\"Caption embedding shape: {cap_embedding.shape}\")\n",
    "        \n",
    "        # Adjust the dimensions of cap_embedding\n",
    "        # Option 1: Squeeze the first dimension (num_captions) if you want to process a single caption\n",
    "        # cap_embedding = cap_embedding.squeeze(1)  # [batch_size, seq_length, embedding_dim]\n",
    "        \n",
    "        # Option 2: Reshape cap_embedding to match the dimensions needed for concatenation\n",
    "        cap_embedding = cap_embedding.view(cap_embedding.size(0), -1, cap_embedding.size(3))  # [batch_size, num_captions*seq_length, embedding_dim]\n",
    "        \n",
    "        # Now, cap_embedding should have 3 dimensions to match img_embedding\n",
    "        embeddings = torch.cat((img_embedding.repeat(1, cap_embedding.size(1), 1), cap_embedding), dim=1)  # Concatenate\n",
    "        \n",
    "        # Pass through LSTM\n",
    "        lstm_out, _ = self.lstm(embeddings)\n",
    "    \n",
    "        # Pass through fully connected layer to get final output\n",
    "        outputs = self.fc(lstm_out)\n",
    "    \n",
    "        return outputs\n"
   ],
   "id": "3072f6c495da57d1",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T08:35:29.215201Z",
     "start_time": "2024-10-19T08:35:29.112718Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the model\n",
    "vocab_size = 8517  # You mentioned the vocabulary size\n",
    "embedding_dim = 256  # Can be adjusted based on your preference\n",
    "hidden_dim = 512     # Size of the hidden state in the RNN/LSTM\n",
    "num_layers = 2       # Number of layers in LSTM\n",
    "dropout_prob = 0.5   # Dropout probability\n",
    "\n",
    "model = ShowTellModel(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_prob)\n",
    "print(\"Model initialized!\")\n"
   ],
   "id": "f724d1a7372ee99e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized!\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T08:35:29.985979Z",
     "start_time": "2024-10-19T08:35:29.974012Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Define the loss function\n",
    "loss_function = nn.CrossEntropyLoss()\n"
   ],
   "id": "65c2cef1e1575bf9",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T08:35:32.369218Z",
     "start_time": "2024-10-19T08:35:30.923902Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ],
   "id": "21da83ceb7e66166",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T08:35:35.180502Z",
     "start_time": "2024-10-19T08:35:33.002121Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_epochs = 10  # Set the number of epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    \n",
    "    for images, captions in train_loader:\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images, captions)\n",
    "        \n",
    "        # After computing the outputs from your model\n",
    "        outputs = outputs.view(-1, outputs.size(-1))  # Shape: [batch_size * sequence_length, vocab_size]\n",
    "        captions = captions.view(-1)  # This will flatten it to [32 * 5 * 21]\n",
    "        print(f\"Outputs shape: {outputs.shape}\")  # Should be [batch_size * sequence_length, vocab_size]\n",
    "        print(f\"Captions shape: {captions.shape}\")  # Should be [batch_size * sequence_length]\n",
    "\n",
    "        loss = loss_function(outputs, captions)\n",
    "\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print the loss for tracking\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")\n"
   ],
   "id": "f84dec2482701c9e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image embedding shape: torch.Size([32, 1, 256])\n",
      "Caption embedding shape: torch.Size([32, 5, 21, 256])\n",
      "Outputs shape: torch.Size([6720, 8517])\n",
      "Captions shape: torch.Size([3360])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (6720) to match target batch_size (3360).",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[15], line 18\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOutputs shape: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00moutputs\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)  \u001B[38;5;66;03m# Should be [batch_size * sequence_length, vocab_size]\u001B[39;00m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCaptions shape: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcaptions\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)  \u001B[38;5;66;03m# Should be [batch_size * sequence_length]\u001B[39;00m\n\u001B[1;32m---> 18\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[43mloss_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcaptions\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;66;03m# Backward pass and optimization\u001B[39;00m\n\u001B[0;32m     22\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[1;32mV:\\Codes\\Python\\ImageCaption\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mV:\\Codes\\Python\\ImageCaption\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mV:\\Codes\\Python\\ImageCaption\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1188\u001B[0m, in \u001B[0;36mCrossEntropyLoss.forward\u001B[1;34m(self, input, target)\u001B[0m\n\u001B[0;32m   1187\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor, target: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m-> 1188\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcross_entropy\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1189\u001B[0m \u001B[43m                           \u001B[49m\u001B[43mignore_index\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mignore_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreduction\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreduction\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1190\u001B[0m \u001B[43m                           \u001B[49m\u001B[43mlabel_smoothing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlabel_smoothing\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mV:\\Codes\\Python\\ImageCaption\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:3104\u001B[0m, in \u001B[0;36mcross_entropy\u001B[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001B[0m\n\u001B[0;32m   3102\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m size_average \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m reduce \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   3103\u001B[0m     reduction \u001B[38;5;241m=\u001B[39m _Reduction\u001B[38;5;241m.\u001B[39mlegacy_get_string(size_average, reduce)\n\u001B[1;32m-> 3104\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_C\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_nn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcross_entropy_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_Reduction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_enum\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreduction\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mignore_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabel_smoothing\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mValueError\u001B[0m: Expected input batch_size (6720) to match target batch_size (3360)."
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1997a2fc52aa0852"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
